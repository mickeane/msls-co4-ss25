{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Digital image processing – Basics**\n",
    "\n",
    "<div style=\"color:#777777;margin-top: -15px;\">\n",
    "<b>Author</b>: Norman Juchler |\n",
    "<b>Course</b>: MSLS CO4 |\n",
    "<b>Version</b>: v1.1 <br><br>\n",
    "<!-- Date: 27.02.2025 -->\n",
    "<!-- Comments: ... -->\n",
    "</div>\n",
    "\n",
    "In this notebook, we will learn how to handle images in Python using the OpenCV and PIL packages. \n",
    "We will also learn how to read, create and modify images using these libraries.\n",
    "\n",
    "<!-- \n",
    "## **Exercises**\n",
    "* [Exercise 1](#exercise1)  \n",
    "* [Exercise 2](#exercise2)  \n",
    "* [Exercise 3](#exercise3)  \n",
    "* [Exercise 4](#exercise4)  \n",
    "* [Exercise 5](#exercise5)  \n",
    "* [Exercise 6](#exercise6)  \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "Let's begin with the usual preparatory steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import PIL\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "# Inline backend configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable this line if you want to use the interactive widgets\n",
    "# It requires the ipympl package to be installed.\n",
    "#%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise1'></a>\n",
    "\n",
    "## **&#9734;  Exercise 1 – Image representation**\n",
    "\n",
    "We have already learned that a digital (grayscale) image is represented as a 2D array of pixel values. In this exercise, we will explore how to load an image and access its pixel values using OpenCV, a powerful library for image processing and computer vision, optimized for real-time applications. Although OpenCV is written in C++, it provides Python bindings for ease of use.\n",
    "\n",
    "A grayscale image is stored as a 2D array of intensity values, where the total number of values is equal to the number of pixels (height $h$ × width $w$). In contrast, a color image is represented as a 3D array with pixel values across multiple channels—for example, an RGB image contains three times as many values as a grayscale image (one set for each color channel: Red, Green, and Blue).\n",
    "\n",
    "The suitable data structure for handling image data in Python is a NumPy array. In fact, most image processing libraries in Python either use NumPy arrays directly or provide functions to convert images into this format. This makes it easy to combine different libraries when working with images.\n",
    "\n",
    "To visualize an image, we can use the [`imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) function from matplotlib.\n",
    "\n",
    "### **Instructions**  \n",
    "\n",
    "1. Read the image `\"../data/images/kingfisher-gray.jpg\"` using `cv.imread()`. See [this example](https://docs.opencv.org/4.x/db/deb/tutorial_display_image.html) for reference.  \n",
    "2. Examine the returned array: What is its shape? What is its data type?  \n",
    "3. Display the image using [`plt.imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html).    \n",
    "   **Hint**: Set the argument `cmap=\"gray\"` to properly display the grayscale image.  \n",
    "4. Retrieve the pixel value at position `(100, 100)`.  \n",
    "5. Select a **region of interest (ROI)**, starting at pixel (75, 200) with a width of 200 and height of 100 pixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "# Read the image\n",
    "gray = cv.imread(...)\n",
    "\n",
    "# Print the image properties\n",
    "print(\"Height, width:       \", ...)\n",
    "print(\"Data type:           \", ...)\n",
    "print()\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(...)\n",
    "plt.show()\n",
    "\n",
    "# Read the pixel at [100,100]\n",
    "print(\"Pixel at [100,100]:  \", ...)\n",
    "\n",
    "# Crop the image and display it\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "# cv.IMREAD_GRAYSCALE: Load the image in grayscale\n",
    "gray = cv.imread(\"../data/images/kingfisher-gray.jpg\", \n",
    "                 cv.IMREAD_GRAYSCALE)\n",
    "print(\"Height, width:\", gray.shape)\n",
    "print(\"Data type:    \", gray.dtype)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(gray, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Pixel gray[100,100]\", gray[100, 100])\n",
    "\n",
    "# Crop the image and display it\n",
    "# (Depending on the context, it may be better to copy the cropped image)\n",
    "gray_cropped = gray[75:175, 200:400].copy()\n",
    "plt.imshow(gray_cropped, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise1'></a>\n",
    "\n",
    "## **&#9734;  Exercise 2 – Digitalization**\n",
    "\n",
    "Just like time-based data, digital images are represented as arrays of numbers. The digitalization/discretization process is very similar and involves two steps:\n",
    "1. Sampling: the continuous image is sampled at regular intervals to obtain a grid of pixels.\n",
    "2. Quantization: the continuous pixel values are quantized to a finite number of levels.\n",
    "\n",
    "The result is a 2D array of numbers, where each number represents the intensity (or color) of a pixel.\n",
    "\n",
    "\n",
    "### **Instructions:** \n",
    "\n",
    "* Explain the main difference(s) between the sampling of continuous-time signals and the spatial sampling of images.\n",
    "* Use the image from the previous exercise\n",
    "* Study the effect of quantization: What happens when you reduce the number of bits?\n",
    "  * Have a look at the function: quantize(image, nbits). Understand how it works.  \n",
    "     Note: NumPy does not have built-in data types for n-bit images, so we use 8-bit  \n",
    "     arrays - but we can still simulate n-bit quantization by rounding the values.\n",
    "  * Use the function `quantize()` to (artificially) create images with a lower image depth\n",
    "* Study the effect of spatial sampling: What happens when you reduce the number of pixels?\n",
    "  * Progressively subsample the image by a factor of two: `gray[::2, ::2]`\n",
    "  * Reflect: What to expect with this type of subsampling, besides getting a lower resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(image, nbits):\n",
    "    \"\"\"Method 1: \n",
    "    Quantize an image to a lower number of bits. The image must be\n",
    "    in uint8 format. The number of bits must be between 1 and 8.\n",
    "    The function works for an arbitrary number of channels.\n",
    "    \"\"\"\n",
    "    assert image.dtype == np.uint8, \"Image must be uint8\"\n",
    "    assert nbits > 0 and nbits <= 8, \"Invalid number of bits\"\n",
    "    \n",
    "    # Create equally sized bins for the 256 levels (per channel).\n",
    "    # np.digitize(img, bins) returns the bin index for each pixel\n",
    "    # value, where bins is an array of bin edges: [bins[i], bins[i+1])\n",
    "    # Note that the first bin with index 0 is [-infinity, bins[0]),\n",
    "    # and the next bin with index 1 is [bins[0], bins[1]). As the \n",
    "    # first bin is irrelevant in our case, we need to subtract 1\n",
    "    # from the returned bin indices (to start from 0).\n",
    "    #\n",
    "    # Examples for the bins, depending on the number of bits: \n",
    "    #   - nbits=8, bins=[0,1,2,...,256]\n",
    "    #   - nbits=2, bins=[0,64,128,192,256]\n",
    "    #   - nbits=1, bins=[0,128,256]\n",
    "    bins = np.arange(0, 256+1, 256 // 2**nbits)\n",
    "    ret = np.digitize(image, bins) - 1\n",
    "    # ret is now an array of bin indices, with the reduced number\n",
    "    # of levels. However, we still store the image in uint8 format,\n",
    "    # so we need to multiply the bin indices with the bin width.\n",
    "    ret *= (256 // 2**nbits)\n",
    "    return ret.astype(np.uint8)\n",
    "\n",
    "\n",
    "# def quantize(image, nbits):\n",
    "#     \"\"\"Method 2:\n",
    "#     Equivalent to the previous function, but using a more efficient\n",
    "#     bitwise operation. The function works for an arbitrary number of\n",
    "#     channels.\"\"\"\n",
    "#     assert image.dtype == np.uint8, \"Image must be uint8\"\n",
    "#     assert nbits > 0 and nbits <= 8, \"Invalid number of bits\"\n",
    "#     return ((image >> (8 - nbits))) << (8 - nbits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Differences between temporal and spatial sampling?\n",
    "\n",
    "# Convert to n-bit image\n",
    "print(\"Effect of quantization\")\n",
    "print(\"######################\")\n",
    "nbits = [8, 6, 4, 3, 2, 1]\n",
    "fig, axes = plt.subplots(2, len(nbits)//2, figsize=(9, 6))\n",
    "for i, nbit in enumerate(nbits):\n",
    "    gray_quantized = ...\n",
    "    ...\n",
    "\n",
    "\n",
    "# Sub-sample the original image\n",
    "print(\"Effect of spatial sampling\")\n",
    "print(\"##########################\")\n",
    "fig, axes = plt.subplots(2, len(nbits)//2, figsize=(9, 6))\n",
    "gray_sub = gray.copy()\n",
    "for i in range(1,6):\n",
    "    gray_sub = ...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "# Differences between temporal and spatial sampling?\n",
    "# - Temporal sampling: Sampling the signal at different time points, \n",
    "#                      The input signal s(t) is a function of time.\n",
    "# - Spatial sampling:  Sampling the signal at different spatial locations\n",
    "#                      The input signal s(x,y) is a function of space (x,y)\n",
    "#                      We sample in two dimensions (x,y) instead of one (t)\n",
    "\n",
    "# Use the image from exercise\n",
    "print(\"Height, width:       \", gray.shape)\n",
    "print(\"Data type:           \", gray.dtype)\n",
    "print(\"Pixel at [100,100]:  \", gray[100, 100])\n",
    "print()\n",
    "\n",
    "# Convert to n-bit image\n",
    "print(\"Effect of quantization\")\n",
    "print(\"######################\")\n",
    "nbits = [8, 6, 4, 3, 2, 1]\n",
    "fig, axes = plt.subplots(2, len(nbits)//2, figsize=(9, 6))\n",
    "for i, nbit in enumerate(nbits):\n",
    "    gray_quantized = quantize(gray, nbit)\n",
    "    axes[i//3, i%3].imshow(gray_quantized, cmap=\"gray\")\n",
    "    axes[i//3, i%3].set_title(f\"{nbit}-bit ({2**nbit} levels)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Sub-sample the original image\n",
    "print(\"Effect of spatial sampling\")\n",
    "print(\"##########################\")\n",
    "fig, axes = plt.subplots(2, len(nbits)//2, figsize=(9, 6))\n",
    "gray_sub = gray.copy()\n",
    "axes[0, 0].imshow(gray, cmap=\"gray\")\n",
    "axes[0, 0].set_title(f\"Original\")\n",
    "for i in range(1,6):\n",
    "    gray_sub = gray_sub[::2, ::2]\n",
    "    axes[i//3, i%3].imshow(gray_sub, cmap=\"gray\")\n",
    "    axes[i//3, i%3].set_title(f\"subsampled {2**i}x\")\n",
    "\n",
    "# Important note: this type of subsampling may cause aliasing effects!\n",
    "# It is better to use proper anti-aliasing filters (=low-pass filters) \n",
    "# when sub-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise3'></a>\n",
    "\n",
    "## **&#9734;  Exercise 3 – Color representation**\n",
    "\n",
    "Gray is dull! Let's use more color in the next exercise. We again read in the same image, but this time in color. Let's try it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color = cv.imread(\"../data/images/kingfisher.jpg\")\n",
    "# A convenient function to display an image directly in the notebook\n",
    "tools.display_image(color)\n",
    "\n",
    "print(\"Height, width, channels:\", color.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is in fact a color image: The image is now represented using 3 channels. But the colors in the image look strange. The reason for this is that OpenCV reads the image in **BGR format**, while matplotlib expects **RGB format**. Shuffling the channels will fix the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_bgr = color.copy()\n",
    "color_rgb = color_bgr[..., ::-1]\n",
    "tools.display_image(color_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instructions**\n",
    "* Examine the image shape and depth\n",
    "* Print the pixel value at (100,100)\n",
    "* Crop the image to the region (125:300, 300:600) and display the cropped image.\n",
    "* Convert the image to an 8-bit grayscale image. Implement at least two conversion methods (hint: see lecture slides)\n",
    "* Use the function [`cv.cvtColor(image, cv.COLOR_RGB2GRAY)`](https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html) to convert the image to grayscale.\n",
    "* Optional task: Use again the quantize function to quantize color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "print(\"Height, width, channels:\", ...)\n",
    "print(\"Data type:              \", ...)\n",
    "print(\"Pixel at [100,100]:     \", ...)\n",
    "print()\n",
    "\n",
    "# Crop the image\n",
    "color_cropped = ...\n",
    "tools.display_image(color_cropped, scale=2.0)\n",
    "\n",
    "# Convert to grayscale:\n",
    "print(\"Grayscale conversion\")\n",
    "print(\"####################\")\n",
    "gray_method1 = ...\n",
    "gray_method2 = ...\n",
    "gray_opencv = ...\n",
    "tools.show_image_grid((color_rgb, gray_method1, gray_method2, gray_opencv),\n",
    "                       titles=(\"Original\", \"Method 1\", \"Method 2\", \"OpenCV\"))\n",
    "\n",
    "# Convert to n-bit image\n",
    "print(\"Effect of quantization\")\n",
    "print(\"######################\")\n",
    "nbits = [8, 6, 4, 3, 2, 1]\n",
    "fig, axes = plt.subplots(2, len(nbits)//2, figsize=(12, 6))\n",
    "for i, nbit in enumerate(nbits):\n",
    "    color_quantized = ...\n",
    "    axes[i//3, i%3].imshow(color_quantized, cmap=\"gray\")\n",
    "    axes[i//3, i%3].set_title(f\"{nbit}-bit (2^{3*nbit} colors)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "print(\"Height, width, channels:\", color.shape)\n",
    "print(\"Data type:              \", color.dtype)\n",
    "print(\"Pixel at [100,100]:     \", color[100, 100])\n",
    "print()\n",
    "\n",
    "# Crop the image\n",
    "color_cropped = color_rgb[75:175, 200:400].copy()\n",
    "tools.display_image(color_cropped, scale=2.0)\n",
    "\n",
    "# Convert to grayscale:\n",
    "print(\"Grayscale conversion\")\n",
    "print(\"####################\")\n",
    "gray_value = color_rgb.max(axis=-1)\n",
    "gray_luminance = (color_rgb.max(axis=-1).astype(np.uint16) + color_rgb.min(axis=-1)) // 2\n",
    "gray_intensity = color_rgb.mean(axis=-1).astype(np.uint8)\n",
    "gray_weighted = (color_rgb @ [0.299, 0.587, 0.114]).astype(np.uint8)\n",
    "gray_opencv = cv.cvtColor(color_rgb, cv.COLOR_RGB2GRAY)\n",
    "tools.show_image_grid((color_rgb, gray_value, gray_luminance, gray_intensity, gray_weighted, gray_opencv),\n",
    "                       titles=(\"Original\", \"Value\", \"Luminance\", \"Intensity\", \"Weighted average\", \"OpenCV\"),\n",
    "                       figsize=(9, 6))\n",
    "\n",
    "# Convert to n-bit image\n",
    "print(\"Effect of quantization\")\n",
    "print(\"######################\")\n",
    "nbits = [8, 6, 4, 3, 2, 1]\n",
    "fig, axes = plt.subplots(2, len(nbits)//2, figsize=(9, 6))\n",
    "for i, nbit in enumerate(nbits):\n",
    "    color_quantized = quantize(color_rgb, nbit)\n",
    "    axes[i//3, i%3].imshow(color_quantized, cmap=\"gray\")\n",
    "    axes[i//3, i%3].set_title(f\"{nbit}-bit (2^{3*nbit} colors)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Digression: Pillow, color bands an dithering**\n",
    "\n",
    "Pillow (PIL) is a powerful alternative to OpenCV for image processing. It has a more user-friendly API and provides more advanced image processing functions. We will use it here and there in this course for specific tasks.\n",
    "\n",
    "Many things can be done fairly easily with the Pillow library. For instance, it comes with its own quantization function [`PIL.Image.quantize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.quantize), and we can even dither the image to improve the perceived quality of the quantized image.\n",
    "\n",
    "In the following, we quantize the image without and with dithering. Notice the colorbands in the quantized image without dithering. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the image from numpy to Pillow. Easy!\n",
    "color_pil = PIL.Image.fromarray(color_rgb)\n",
    "\n",
    "\n",
    "# Quantize without dithering\n",
    "ncolors = [256, 128, 64, 32, 16, 8]\n",
    "images = []\n",
    "for i, ncols in enumerate(ncolors):\n",
    "    img = color_pil.quantize(colors=ncols,\n",
    "                             method=PIL.Image.Quantize.MAXCOVERAGE,\n",
    "                             kmeans=1, )\n",
    "    images.append(img.convert(\"RGB\"))\n",
    "\n",
    "# Convenience function to plot a list of images in a grid.\n",
    "tools.show_image_grid(images, titles=[f\"{nc} colors\" for nc in ncolors],\n",
    "                      figsize=(9, 6), ncols=3)\n",
    "\n",
    "\n",
    "# Quantize with dithering, which is available only for paletted images. \n",
    "# \n",
    "# Palette images are images with a limited number of colors, where each pixel\n",
    "# value is an index to the color palette. Pillow can infer the palette from \n",
    "# the image (palette=PIL.Palette.ADAPTIVE) or it uses the Web palette of \n",
    "# 256 web-safe colors (palette=PIL.Palette.WEB).\n",
    "ncolors = [256, 128, 64, 32, 16, 8]\n",
    "images = []\n",
    "for i, ncols in enumerate(ncolors):\n",
    "    # Convert to a paletted image with the maximum coverage method\n",
    "    img = color_pil.convert(\"P\", dither=None, palette=PIL.Image.Palette.ADAPTIVE)\n",
    "    #img = color_pil.convert(\"P\", dither=None, palette=PIL.Image.Palette.WEB)\n",
    "    img = img.quantize(colors=ncols, \n",
    "                       method=PIL.Image.Quantize.MAXCOVERAGE, \n",
    "                       kmeans=1, \n",
    "                       dither=PIL.Image.Dither.FLOYDSTEINBERG)\n",
    "    images.append(img.convert(\"RGB\"))\n",
    "\n",
    "tools.show_image_grid(images, titles=[f\"{nc} colors\" for nc in ncolors],\n",
    "                      figsize=(9, 6), ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise4'></a>\n",
    "\n",
    "## **&#9734;  Exercise 4 – Construct an image**\n",
    "\n",
    "You can construct an image yourselve. In this exercise, you need to create a checkerboard consisting of 8x8 squares, where each square is 32x32 pixels. The squares should be colored alternately with any two colors (e.g. red and green or black and white). The final image should be 256x256 pixels.\n",
    "\n",
    "Complete the following function template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "def create_checkerboard(square_size=32, checker_size=8, color1=[0,0,0], color2=[255,255,255]):\n",
    "    \"\"\"Create a checkerboard image with the specified square size and checker size.\"\"\"\n",
    "    image_size = square_size * checker_size\n",
    "    # Complete this function\n",
    "    iamge = ...\n",
    "    return image\n",
    "    \n",
    "checker = create_checkerboard(square_size=32, \n",
    "                              checker_size=8, \n",
    "                              color1=[255,188,188], \n",
    "                              color2=[255,255,188]) \n",
    "tools.display_image(checker, scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "def create_checkerboard(square_size=32, checker_size=8, color1=[0,0,0], color2=[255,255,255]):\n",
    "    \"\"\"Create a checkerboard image with the specified square size and checker size.\n",
    "    This version uses a naive approach with nested loops.\"\"\"\n",
    "    # Create an empty array for the image\n",
    "    image_size = square_size * checker_size\n",
    "    image = np.zeros((image_size, image_size, 3), dtype=np.uint8)\n",
    "    # Iterate over each square and fill with the specified colors\n",
    "    for i in range(0, image_size, square_size):\n",
    "        for j in range(0, image_size, square_size):\n",
    "            if (i // square_size + j // square_size) % 2 == 0:\n",
    "                color = color1\n",
    "            else:\n",
    "                color = color2\n",
    "            image[i:i+square_size, j:j+square_size] = color\n",
    "    return image\n",
    "\n",
    "def create_checkerboard(square_size=32, checker_size=8, color1=[0,0,0], color2=[255,255,255]):\n",
    "    \"\"\"Create a checkerboard image with the specified square size and checker size.\n",
    "    This version uses a more efficient tiling approach.\"\"\"\n",
    "    image_size = square_size * checker_size\n",
    "    tile = np.array([[color1,color2],[color2,color1]]).repeat(square_size, axis=0).repeat(square_size, axis=1)\n",
    "    image = np.tile(tile, (checker_size // 2 + 1, checker_size // 2 + 1, 1))\n",
    "    return image[:image_size,:image_size].astype(np.uint8)\n",
    "\n",
    "    \n",
    "checker = create_checkerboard(square_size=32, \n",
    "                              checker_size=8, \n",
    "                              color1=[255,188,188], \n",
    "                              color2=[255,255,188]) \n",
    "tools.display_image(checker, scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise5'></a>\n",
    "\n",
    "## **&#9734;  Exercise 5 – Gamma correction**\n",
    "\n",
    "Gamma correction is a non-linear operation to adjust the brightness of an image. It is used to correct the non-linear response of the human eye to light intensity, or the non-linear behavior of certain display devices.\n",
    "\n",
    "The gamma correction is defined as:\n",
    "\n",
    "$$I_{out} = I_{in} ^ {\\;\\gamma}$$\n",
    "\n",
    "Here, $\\gamma > 0$ is the correction factor. For $\\gamma=1$, the image remains unchanged. For $\\gamma>1$, the image is darkened, and for $\\gamma<1$, the image is brightened.\n",
    "\n",
    "**Hint**: The operation requires the image to be in float format. In this format, the pixel values should be in the range [0,1]. That means, besides changing the dtype to float, the pixel values should be normalized to the range [0,1]. Matplotlib can handle floating point images just like images with integral depth.\n",
    "\n",
    "\n",
    "### **Instruction:**\n",
    "Complete the following function template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Solution\n",
    "def gamma_correction(image, gamma):\n",
    "    \"\"\"Apply gamma correction to an image with the specified gamma value.\"\"\"\n",
    "    # Conver to float if the image is in uint8 format\n",
    "    image = ...\n",
    "    # Apply the gamma correction\n",
    "    corrected = ...\n",
    "    return corrected\n",
    "\n",
    "# Test the function.\n",
    "image = color_rgb.copy()\n",
    "gamma = 1.5\n",
    "corrected = gamma_correction(image, gamma)\n",
    "tools.show_image_grid((image, corrected), titles=(\"Original\", f\"Gamma={gamma}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "def gamma_correction(image, gamma):\n",
    "    \"\"\"Apply gamma correction to an image with the specified gamma value.\n",
    "    The function works for an arbitrary number of channels and for both uint8 and float images.\"\"\"\n",
    "    assert gamma > 0, \"Gamma must be positive\"\n",
    "    # Convert to float if the image is in uint8 format\n",
    "    image = image.astype(np.float32) / 255.0 if image.dtype == np.uint8 else image\n",
    "    # Apply the gamma correction\n",
    "    corrected = image ** gamma\n",
    "    return corrected\n",
    "\n",
    "# Test the function.\n",
    "image = color_rgb.copy()\n",
    "#image = gray.copy()\n",
    "gammas = [2.0, 1.5, 1.0, 0.5, 0.25]\n",
    "titles = [\"gamma=2.0\", \"gamma=1.5\", \"original\", \"gamma=0.5\", \"gamma=0.25\"]\n",
    "results = []\n",
    "for gamma, title in zip(gammas, titles):\n",
    "    corrected = gamma_correction(image, gamma)\n",
    "    results.append(corrected)\n",
    "tools.show_image_grid(results, titles=titles, ncols=-1, \n",
    "                      figsize=(9, 3), suppress_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise6'></a>\n",
    "\n",
    "## **&#9734;  Exercise 6 – Color and intensity histograms**\n",
    "\n",
    "### **Instructions**\n",
    "* Read the following section carefully \n",
    "* Make sure you understand the concept of intensity and color histograms\n",
    "* Understand the type of color transformations we examine here\n",
    "\n",
    "<br>\n",
    "\n",
    "The *distribution of intensity* values in the image (or channel) is an relevant characteristic of the image. The *histogram* of an image shows the number of pixels for each intensity value. It is a useful visualization to understand the contrast, brightness, and dynamic range of an image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the histogram of the image\n",
    "hist, bins = np.histogram(gray.flatten(), bins=256, range=[0,256], density=True)\n",
    "cdf = hist.cumsum()\n",
    "hist /= hist.max()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "axes[0].imshow(gray, cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].plot(hist, label=\"Histogram (normalized)\")\n",
    "axes[1].plot(cdf, label=\"CDF\")\n",
    "axes[1].set_xlabel(\"Pixel value\")\n",
    "axes[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe for this particular image that most of the pixel values are concentrated between 120 and 180, while there are almost no pixels with values below 100.\n",
    "\n",
    "The above plot displays also the cumulative distribution function (CDF) of the pixel values. The CDF shows the fraction of pixels with values less than or equal to a given value. For example, the CDF value at 100 is < 0.1, which means that less than 10% of the pixels have values less than 100.\n",
    "\n",
    "The situation seen in the above plot can be considered as a low-contrast image, where the pixel values are concentrated in a narrow range. This can be improved by applying a contrast stretching operation, which maps the pixel values to a wider range. One such approach is called **histogram equalization**, which is a method to improve the contrast of an image by equalizing the histogram. The goal is to obtain a uniform histogram, where all pixel values are equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply histogram equalization\n",
    "gray = cv.imread(\"../data/images/kingfisher-gray.jpg\", cv.IMREAD_GRAYSCALE)\n",
    "gray_equalized = cv.equalizeHist(gray)\n",
    "hist_equalized, bins = np.histogram(gray_equalized.flatten(), \n",
    "                                    bins=256, range=[0,256], \n",
    "                                    density=True)\n",
    "cdf_equalized = hist_equalized.cumsum()\n",
    "hist_equalized /= hist_equalized.max()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "axes[0].imshow(gray_equalized, cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].plot(hist_equalized, label=\"Histogram (normalized)\")\n",
    "axes[1].plot(cdf_equalized, label=\"CDF\")\n",
    "axes[1].set_xlabel(\"Pixel value\")\n",
    "axes[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equalized image has a more uniform histogram, which means that the pixel values are more evenly distributed. The CDF is also more linear. All this indicates that the contrast of the image has been improved. On the other hand, the equalization may also amplify the noise or other distortions in the image. For instance, the quantization bands in the original image are more pronounced in the equalized image.\n",
    "\n",
    "For color images, we can apply histogram equalization to each channel separately. We can also use the YUV color space, where the Y channel represents the luminance (brightness) of the image. We can apply histogram equalization to the Y channel only, and then convert back to RGB. The following code demonstrates this approach for the YUV and also the HSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to visualize the histogram of all 3 channels in the image\n",
    "def visualize_histogram(image, title=\"Histogram\"):\n",
    "\n",
    "    def visualize_channel(img, title, ax, color):\n",
    "        hist, bins = np.histogram(img.flatten(), bins=256, range=[0,256], density=True)\n",
    "        cdf = hist.cumsum()\n",
    "        hist /= hist.max()\n",
    "        ax.plot(hist, label=\"Histogram (normalized)\", color=color)\n",
    "        ax.plot(cdf, label=\"CDF\", color=\"k\", linestyle=\":\")\n",
    "        if False:\n",
    "            ax.set_xlabel(\"Pixel value\")\n",
    "            ax.legend()\n",
    "        ax.set_title(title)\n",
    "\n",
    "    \"\"\"Visualize the histogram of an image.\"\"\"\n",
    "    if image.ndim == 1:\n",
    "        return visualize_channel(image, title, plt.gca())\n",
    "    else:\n",
    "        nchannels = image.shape[-1]\n",
    "        fig, axes = plt.subplots(1, nchannels+1, figsize=(9, 2))\n",
    "        axes[0].imshow(image, cmap=\"gray\" if nchannels == 1 else None)\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[0].set_title(title)\n",
    "        axes[0].set_anchor(\"N\")\n",
    "        titles = [\"R channel\", \"G channel\", \"B channel\"]\n",
    "        for i in range(image.shape[-1]):\n",
    "            visualize_channel(image[..., i], \n",
    "                              title=titles[i],\n",
    "                              ax=axes[i+1], color=tools.PALETTE_RGB[i])\n",
    "    #fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Convert the image to float\n",
    "color_equalized = np.stack([cv.equalizeHist(color_bgr[...,i]) for i in range(3)], axis=2)\n",
    "visualize_histogram(color_rgb, title=\"Original\")\n",
    "visualize_histogram(color_equalized, title=\"Equalized RGB\")\n",
    "# Better, convert to HSL color space and apply histogram equalization to the Y channel\n",
    "color_hls = cv.cvtColor(color_rgb, cv.COLOR_RGB2HLS)\n",
    "color_hls[..., 1] = cv.equalizeHist(color_hls[..., 1])\n",
    "color_equalized = cv.cvtColor(color_hls, cv.COLOR_HLS2RGB)\n",
    "visualize_histogram(color_equalized, title=\"Equalized lum. (HLS)\")\n",
    "# Also the YUV color space can be used\n",
    "color_hls = cv.cvtColor(color_rgb, cv.COLOR_RGB2YUV)\n",
    "color_hls[..., 0] = cv.equalizeHist(color_hls[..., 0])\n",
    "color_equalized = cv.cvtColor(color_hls, cv.COLOR_YUV2RGB)\n",
    "visualize_histogram(color_equalized, title=\"Equalized lum. (YUV)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: The histogram equalization method can also be applied to color images. However, applying the method to each channel independently leads to color distortions. Instead, the method should be applied to a single channel, which represents the luminance or intensity of the image. The luminance channel can be extracted from the RGB image using different color spaces, such as HLS or YUV. This results in more appealing pictures.\n",
    "\n",
    "Histogram equalization is a simple and effective method to improve the contrast of an image. \n",
    "\n",
    "**Histogram matching** is another method to adjust the contrast of an image by matching the histogram of the input image to a reference histogram. Here we use the histogram matching method implemented in the scikit-image library. The function [`skimage.exposure.match_histograms()`](https://scikit-image.org/docs/stable/api/skimage.exposure.html=) takes the input image and the reference image as input and returns the matched image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skimage import exposure\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "color1 = color_rgb.copy()\n",
    "color2 = cv.imread(\"../data/images/veggies.jpg\")\n",
    "color2 = cv.cvtColor(color2, cv.COLOR_BGR2RGB)\n",
    "color3 = cv.imread(\"../data/images/flowers.jpg\")\n",
    "color3 = cv.cvtColor(color3, cv.COLOR_BGR2RGB)\n",
    "\n",
    "matched12 = match_histograms(color1, color2, channel_axis=-1)\n",
    "matched23 = match_histograms(color1, color3, channel_axis=-1)\n",
    "\n",
    "tools.show_image_grid((color1, color2, matched12), \n",
    "                       titles=(\"Input\", \"Reference\", \"Matched\"),\n",
    "                       figsize=(9, 6))\n",
    "tools.show_image_grid((color1, color3, matched23), \n",
    "                       titles=(\"Input\", \"Reference\", \"Matched\"),\n",
    "                       figsize=(9, 6))\n",
    "\n",
    "visualize_histogram(color_rgb, title=\"Input\")\n",
    "visualize_histogram(color3, title=\"Input\")\n",
    "visualize_histogram(matched23, title=\"Matched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the CDFs of the matched image resemble the CDF of the reference image much better!\n",
    "\n",
    "\n",
    "Using histograms, it possible to see what certain color or **intensity adjustments** do to the image. Below, the effect of the following operations is demonstrated:\n",
    "\n",
    "* No operation (copy image)\n",
    "* Inversion\n",
    "* Increase / decrease brightness\n",
    "* Stretch / squeeze contrast\n",
    "* Clipping (ensure values between [min, max])\n",
    "* Binarization of the image (thresholding)\n",
    "* Ggamma correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "lookup_tables = []\n",
    "image = gray.copy()\n",
    "results = []\n",
    "\n",
    "labels.append(\"Identity\")\n",
    "lookup_table = np.arange(256)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Inverted\")\n",
    "lookup_table = 255 - np.arange(256)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Increase brightness\")\n",
    "lookup_table = np.clip(np.arange(256) + 50, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Decrease brightness\")\n",
    "lookup_table = np.clip(np.arange(256) - 50, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Contrast stretch\")\n",
    "# Lookup such that mean remains equal, but the range is stretched\n",
    "# (The peak of the histogram is at 163)\n",
    "center = 163\n",
    "lookup_table = np.clip((np.arange(256) - center) * 3 + center, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Contrast squeeze\")\n",
    "lookup_table = np.clip((np.arange(256) - center) / 6 + center, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Clipping\")\n",
    "lookup_table = np.clip(np.arange(256), 135, 175)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Threshold\")\n",
    "lookup_table = (np.arange(256) > 128) * 255\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Gamma=0.5\")\n",
    "lookup_table = (np.arange(256) / 255) ** 0.5 * 255\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Gamma=2.0\")\n",
    "lookup_table = (np.arange(256) / 255) ** 2.0 * 255\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "\n",
    "# Visualize the results\n",
    "for label, lookup in zip(labels, lookup_tables):\n",
    "    result = lookup[image]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    # Display the result\n",
    "    axes[0].imshow(result, cmap=\"gray\")\n",
    "    axes[0].set_title(\"Operation: \"+label)\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].set_anchor(\"N\")\n",
    "    # Display lookup table\n",
    "    axes[1].plot(lookup)\n",
    "    axes[1].set_title(\"Lookup table\")\n",
    "    axes[1].set_xlabel(\"Input value\")\n",
    "    axes[1].set_ylabel(\"Output value\")\n",
    "    axes[1].set_xlim([0-5, 255+5])\n",
    "    axes[1].set_ylim([0-5, 255+5])\n",
    "    axes[1].set_aspect(\"equal\")\n",
    "    # Display the histograms (before, after)\n",
    "    hist, bins = np.histogram(image.flatten(), bins=256, range=[0,256], density=True)\n",
    "    axes[2].plot(hist, label=\"Before\")\n",
    "    hist, bins = np.histogram(result.flatten(), bins=256, range=[0,256], density=True)\n",
    "    axes[2].plot(hist, label=\"After\")\n",
    "    axes[2].set_title(\"Histogram\")\n",
    "    axes[2].legend()\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-msls-co4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
